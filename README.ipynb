{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch/ACAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch/ACAP is a prototype PyTorch backend targeting Xilinx ACAP devices.\n",
    "It is designed to be a just-in-time compiler (JIT) front-end as well as a PyTorch to MLIR exporter.\n",
    "PyTorch/ACAP closely follows the design of [PyTorch/XLA](http://github.com/pytorch/xla).\n",
    "\n",
    "PyTorch/ACAP currently implements:\n",
    " - A PyTorch ACAP device type and JIT front-end.\n",
    " - An MLIR dialect for expressing PyTorch ATen operations.\n",
    " - MLIR passes for ATen dialect analysis and lowering.\n",
    " \n",
    "The PyTorch ACAP device allows existing PyTorch scripts to target an ACAP device as easily as a GPU or CPU device.  The MLIR dialect export functionality allows MLIR and LLVM based compilers to be used as the optimizer and code generator for the PyTorch/ACAP JIT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLIR Export Example\n",
    "Here we demonstrate the MLIR generation and export capability of the PyTorch/ACAP.  The following is a script containing a very simple PyTorch Model, it just adds two tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_acap\n",
    "\n",
    "class adder(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(adder, self).__init__()\n",
    "        def forward(self, in0, in1):\n",
    "            return in0 + in1\n",
    "\n",
    "cpu_model = adder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run the model and get a result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.5024, -0.1082, -0.4396],\n",
      "         [-1.5719,  0.1685, -0.6653]]])\n"
     ]
    }
   ],
   "source": [
    "cpu_tensor = torch.randn(1,2,3)\n",
    "cpu_result = cpu_model(cpu_tensor, cpu_tensor)\n",
    "print(cpu_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also move the model and data to the ACAP device just like we would when using a GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default ACAP device\n",
    "dev = torch_acap.acap_device()\n",
    "\n",
    "# move the data to the acap device\n",
    "acap_tensor = cpu_tensor.to(dev)\n",
    "acap_model = adder().to(dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run the model as before.  But instead of printing the result tensor, we'll ask the system for the MLIR corresponding to the computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "module {\n",
      "  func @graph(%arg0: tensor<1x2x3xf32>, %arg1: tensor<1x2x3xf32>) -> tensor<1x2x3xf32> {\n",
      "    %0 = \"aten.constant\"() {type = \"i32\", value = 1 : i32} : () -> i32\n",
      "    %1 = \"aten.add\"(%arg0, %arg1, %0) : (tensor<1x2x3xf32>, tensor<1x2x3xf32>, i32) -> tensor<1x2x3xf32>\n",
      "    return %1 : tensor<1x2x3xf32>\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "acap_result = acap_model(acap_tensor, acap_tensor)\n",
    "\n",
    "mlir = torch_acap.get_mlir( acap_result )\n",
    "\n",
    "print(mlir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ACAP device implements a lazy JIT compiler front-end.  That is, it will continue adding to the MLIR until forced to compile.\n",
    "\n",
    "For example, we can continue adding to the above compute graph..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "module {\n",
      "  func @graph(%arg0: tensor<1x2x3xf32>, %arg1: tensor<1x2x3xf32>, %arg2: tensor<1x2x3xf32>, %arg3: tensor<1x2x3xf32>) -> tensor<1x2x3xf32> {\n",
      "    %0 = \"aten.constant\"() {type = \"i32\", value = 1 : i32} : () -> i32\n",
      "    %1 = \"aten.add\"(%arg0, %arg1, %0) : (tensor<1x2x3xf32>, tensor<1x2x3xf32>, i32) -> tensor<1x2x3xf32>\n",
      "    %2 = \"aten.constant\"() {type = \"i32\", value = 1 : i32} : () -> i32\n",
      "    %3 = \"aten.add\"(%1, %arg2, %2) : (tensor<1x2x3xf32>, tensor<1x2x3xf32>, i32) -> tensor<1x2x3xf32>\n",
      "    %4 = \"aten.mul\"(%3, %arg3) : (tensor<1x2x3xf32>, tensor<1x2x3xf32>) -> tensor<1x2x3xf32>\n",
      "    return %4 : tensor<1x2x3xf32>\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# a second add operation...\n",
    "acap_result = acap_result + torch.randn(1,2,3,device=dev)\n",
    "\n",
    "# ...and a mul operation\n",
    "acap_result = acap_result * torch.randn(1,2,3,device=dev)\n",
    "\n",
    "mlir = torch_acap.get_mlir( acap_result )\n",
    "print(mlir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch/ACAP JIT Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
