// RUN: aten-opt %s -aten-to-xten | FileCheck %s
module attributes {torch.debug_module_name = "Model"} {
  func @forward(%arg0: !torch.vtensor<[1,3,128,128],f32>) -> !torch.vtensor<[1,1000],f32> {
    %0 = torch.vtensor.literal(dense<"0xDEADBEEF"> : tensor<1000xf32>) : !torch.vtensor<[1000],f32>
    %1 = torch.vtensor.literal(dense<"0xDEADBEEF"> : tensor<1000x2048xf32>) : !torch.vtensor<[1000,2048],f32>
    %2 = torch.vtensor.literal(dense<0.000000e+00> : tensor<2048xf32>) : !torch.vtensor<[2048],f32>
    %3 = torch.vtensor.literal(dense<1.000000e+00> : tensor<2048xf32>) : !torch.vtensor<[2048],f32>
    %none = torch.constant.none
    %4 = torch.vtensor.literal(dense<"0xDEADBEEF"> : tensor<2048x512x1x1xf32>) : !torch.vtensor<[2048,512,1,1],f32>
    %5 = torch.vtensor.literal(dense<0.000000e+00> : tensor<512xf32>) : !torch.vtensor<[512],f32>
    %6 = torch.vtensor.literal(dense<1.000000e+00> : tensor<512xf32>) : !torch.vtensor<[512],f32>
    %7 = torch.vtensor.literal(dense<"0xDEADBEEF"> : tensor<512x512x3x3xf32>) : !torch.vtensor<[512,512,3,3],f32>
    %8 = torch.vtensor.literal(dense<"0xDEADBEEF"> : tensor<512x2048x1x1xf32>) : !torch.vtensor<[512,2048,1,1],f32>
    %9 = torch.vtensor.literal(dense<"0xDEADBEEF"> : tensor<2048x512x1x1xf32>) : !torch.vtensor<[2048,512,1,1],f32>
    %10 = torch.vtensor.literal(dense<"0xDEADBEEF"> : tensor<512x512x3x3xf32>) : !torch.vtensor<[512,512,3,3],f32>
    %11 = torch.vtensor.literal(dense<"0xDEADBEEF"> : tensor<512x2048x1x1xf32>) : !torch.vtensor<[512,2048,1,1],f32>
    %12 = torch.vtensor.literal(dense<"0xDEADBEEF"> : tensor<2048x1024x1x1xf32>) : !torch.vtensor<[2048,1024,1,1],f32>
    %13 = torch.vtensor.literal(dense<"0xDEADBEEF"> : tensor<2048x512x1x1xf32>) : !torch.vtensor<[2048,512,1,1],f32>
    %14 = torch.vtensor.literal(dense<"0xDEADBEEF"> : tensor<512x512x3x3xf32>) : !torch.vtensor<[512,512,3,3],f32>
    %15 = torch.vtensor.literal(dense<"0xDEADBEEF"> : tensor<512x1024x1x1xf32>) : !torch.vtensor<[512,1024,1,1],f32>
    %16 = torch.vtensor.literal(dense<0.000000e+00> : tensor<1024xf32>) : !torch.vtensor<[1024],f32>
    %17 = torch.vtensor.literal(dense<1.000000e+00> : tensor<1024xf32>) : !torch.vtensor<[1024],f32>
    %18 = torch.vtensor.literal(dense<"0xDEADBEEF"> : tensor<1024x256x1x1xf32>) : !torch.vtensor<[1024,256,1,1],f32>
    %19 = torch.vtensor.literal(dense<0.000000e+00> : tensor<256xf32>) : !torch.vtensor<[256],f32>
    %20 = torch.vtensor.literal(dense<1.000000e+00> : tensor<256xf32>) : !torch.vtensor<[256],f32>
    %21 = torch.vtensor.literal(dense<"0xDEADBEEF"> : tensor<256x256x3x3xf32>) : !torch.vtensor<[256,256,3,3],f32>
    %22 = torch.vtensor.literal(dense<"0xDEADBEEF"> : tensor<256x1024x1x1xf32>) : !torch.vtensor<[256,1024,1,1],f32>
    %23 = torch.vtensor.literal(dense<"0xDEADBEEF"> : tensor<1024x256x1x1xf32>) : !torch.vtensor<[1024,256,1,1],f32>
    %24 = torch.vtensor.literal(dense<"0xDEADBEEF"> : tensor<256x256x3x3xf32>) : !torch.vtensor<[256,256,3,3],f32>
    %25 = torch.vtensor.literal(dense<"0xDEADBEEF"> : tensor<256x1024x1x1xf32>) : !torch.vtensor<[256,1024,1,1],f32>
    %26 = torch.vtensor.literal(dense<"0xDEADBEEF"> : tensor<1024x256x1x1xf32>) : !torch.vtensor<[1024,256,1,1],f32>
    %27 = torch.vtensor.literal(dense<"0xDEADBEEF"> : tensor<256x256x3x3xf32>) : !torch.vtensor<[256,256,3,3],f32>
    %28 = torch.vtensor.literal(dense<"0xDEADBEEF"> : tensor<256x1024x1x1xf32>) : !torch.vtensor<[256,1024,1,1],f32>
    %29 = torch.vtensor.literal(dense<"0xDEADBEEF"> : tensor<1024x256x1x1xf32>) : !torch.vtensor<[1024,256,1,1],f32>
    %30 = torch.vtensor.literal(dense<"0xDEADBEEF"> : tensor<256x256x3x3xf32>) : !torch.vtensor<[256,256,3,3],f32>
    %31 = torch.vtensor.literal(dense<"0xDEADBEEF"> : tensor<256x1024x1x1xf32>) : !torch.vtensor<[256,1024,1,1],f32>
    %32 = torch.vtensor.literal(dense<"0xDEADBEEF"> : tensor<1024x256x1x1xf32>) : !torch.vtensor<[1024,256,1,1],f32>
    %33 = torch.vtensor.literal(dense<"0xDEADBEEF"> : tensor<256x256x3x3xf32>) : !torch.vtensor<[256,256,3,3],f32>
    %34 = torch.vtensor.literal(dense<"0xDEADBEEF"> : tensor<256x1024x1x1xf32>) : !torch.vtensor<[256,1024,1,1],f32>
    %35 = torch.vtensor.literal(dense<"0xDEADBEEF"> : tensor<1024x512x1x1xf32>) : !torch.vtensor<[1024,512,1,1],f32>
    %36 = torch.vtensor.literal(dense<"0xDEADBEEF"> : tensor<1024x256x1x1xf32>) : !torch.vtensor<[1024,256,1,1],f32>
    %37 = torch.vtensor.literal(dense<"0xDEADBEEF"> : tensor<256x256x3x3xf32>) : !torch.vtensor<[256,256,3,3],f32>
    %38 = torch.vtensor.literal(dense<"0xDEADBEEF"> : tensor<256x512x1x1xf32>) : !torch.vtensor<[256,512,1,1],f32>
    %39 = torch.vtensor.literal(dense<"0xDEADBEEF"> : tensor<512x128x1x1xf32>) : !torch.vtensor<[512,128,1,1],f32>
    %40 = torch.vtensor.literal(dense<0.000000e+00> : tensor<128xf32>) : !torch.vtensor<[128],f32>
    %41 = torch.vtensor.literal(dense<1.000000e+00> : tensor<128xf32>) : !torch.vtensor<[128],f32>
    %42 = torch.vtensor.literal(dense<"0xDEADBEEF"> : tensor<128x128x3x3xf32>) : !torch.vtensor<[128,128,3,3],f32>
    %43 = torch.vtensor.literal(dense<"0xDEADBEEF"> : tensor<128x512x1x1xf32>) : !torch.vtensor<[128,512,1,1],f32>
    %44 = torch.vtensor.literal(dense<"0xDEADBEEF"> : tensor<512x128x1x1xf32>) : !torch.vtensor<[512,128,1,1],f32>
    %45 = torch.vtensor.literal(dense<"0xDEADBEEF"> : tensor<128x128x3x3xf32>) : !torch.vtensor<[128,128,3,3],f32>
    %46 = torch.vtensor.literal(dense<"0xDEADBEEF"> : tensor<128x512x1x1xf32>) : !torch.vtensor<[128,512,1,1],f32>
    %47 = torch.vtensor.literal(dense<"0xDEADBEEF"> : tensor<512x128x1x1xf32>) : !torch.vtensor<[512,128,1,1],f32>
    %48 = torch.vtensor.literal(dense<"0xDEADBEEF"> : tensor<128x128x3x3xf32>) : !torch.vtensor<[128,128,3,3],f32>
    %49 = torch.vtensor.literal(dense<"0xDEADBEEF"> : tensor<128x512x1x1xf32>) : !torch.vtensor<[128,512,1,1],f32>
    %50 = torch.vtensor.literal(dense<"0xDEADBEEF"> : tensor<512x256x1x1xf32>) : !torch.vtensor<[512,256,1,1],f32>
    %51 = torch.vtensor.literal(dense<"0xDEADBEEF"> : tensor<512x128x1x1xf32>) : !torch.vtensor<[512,128,1,1],f32>
    %52 = torch.vtensor.literal(dense<"0xDEADBEEF"> : tensor<128x128x3x3xf32>) : !torch.vtensor<[128,128,3,3],f32>
    %53 = torch.vtensor.literal(dense<"0xDEADBEEF"> : tensor<128x256x1x1xf32>) : !torch.vtensor<[128,256,1,1],f32>
    %54 = torch.vtensor.literal(dense<"0xDEADBEEF"> : tensor<256x64x1x1xf32>) : !torch.vtensor<[256,64,1,1],f32>
    %55 = torch.vtensor.literal(dense<0.000000e+00> : tensor<64xf32>) : !torch.vtensor<[64],f32>
    %56 = torch.vtensor.literal(dense<1.000000e+00> : tensor<64xf32>) : !torch.vtensor<[64],f32>
    %57 = torch.vtensor.literal(dense<"0xDEADBEEF"> : tensor<64x64x3x3xf32>) : !torch.vtensor<[64,64,3,3],f32>
    %58 = torch.vtensor.literal(dense<"0xDEADBEEF"> : tensor<64x256x1x1xf32>) : !torch.vtensor<[64,256,1,1],f32>
    %59 = torch.vtensor.literal(dense<"0xDEADBEEF"> : tensor<256x64x1x1xf32>) : !torch.vtensor<[256,64,1,1],f32>
    %60 = torch.vtensor.literal(dense<"0xDEADBEEF"> : tensor<64x64x3x3xf32>) : !torch.vtensor<[64,64,3,3],f32>
    %61 = torch.vtensor.literal(dense<"0xDEADBEEF"> : tensor<64x256x1x1xf32>) : !torch.vtensor<[64,256,1,1],f32>
    %62 = torch.vtensor.literal(dense<"0xDEADBEEF"> : tensor<256x64x1x1xf32>) : !torch.vtensor<[256,64,1,1],f32>
    %63 = torch.vtensor.literal(dense<"0xDEADBEEF"> : tensor<256x64x1x1xf32>) : !torch.vtensor<[256,64,1,1],f32>
    %64 = torch.vtensor.literal(dense<"0xDEADBEEF"> : tensor<64x64x3x3xf32>) : !torch.vtensor<[64,64,3,3],f32>
    %65 = torch.vtensor.literal(dense<"0xDEADBEEF"> : tensor<64x64x1x1xf32>) : !torch.vtensor<[64,64,1,1],f32>
    %66 = torch.vtensor.literal(dense<"0xDEADBEEF"> : tensor<64x3x7x7xf32>) : !torch.vtensor<[64,3,7,7],f32>
    %int-1 = torch.constant.int -1
    %int1 = torch.constant.int 1
    %int2 = torch.constant.int 2
    %float1.000000e-01 = torch.constant.float 1.000000e-01
    %float1.000000e-05 = torch.constant.float 1.000000e-05
    %int0 = torch.constant.int 0
    %int3 = torch.constant.int 3
    %false = torch.constant.bool false
    %true = torch.constant.bool true
    %67 = torch.prim.ListConstruct %int2, %int2 : (!torch.int, !torch.int) -> !torch.list<int>
    %68 = torch.prim.ListConstruct %int3, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %69 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %70 = torch.aten.conv2d %arg0, %66, %none, %67, %68, %69, %int1 : !torch.vtensor<[1,3,128,128],f32>, !torch.vtensor<[64,3,7,7],f32>, !torch.none, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.int -> !torch.vtensor<[1,64,64,64],f32>
    %71 = torch.aten.batch_norm %70, %56, %55, %55, %56, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[1,64,64,64],f32>, !torch.vtensor<[64],f32>, !torch.vtensor<[64],f32>, !torch.vtensor<[64],f32>, !torch.vtensor<[64],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[1,64,64,64],f32>
    %72 = torch.aten.relu %71 : !torch.vtensor<[1,64,64,64],f32> -> !torch.vtensor<[1,64,64,64],f32>
    %73 = torch.aten.max_pool2d %72, %68, %67, %69, %69, %false : !torch.vtensor<[1,64,64,64],f32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,64,32,32],f32>
    %74 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %75 = torch.aten.conv2d %73, %65, %none, %69, %74, %69, %int1 : !torch.vtensor<[1,64,32,32],f32>, !torch.vtensor<[64,64,1,1],f32>, !torch.none, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.int -> !torch.vtensor<[1,64,32,32],f32>
    %76 = torch.aten.batch_norm %75, %56, %55, %55, %56, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[1,64,32,32],f32>, !torch.vtensor<[64],f32>, !torch.vtensor<[64],f32>, !torch.vtensor<[64],f32>, !torch.vtensor<[64],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[1,64,32,32],f32>
    %77 = torch.aten.relu %76 : !torch.vtensor<[1,64,32,32],f32> -> !torch.vtensor<[1,64,32,32],f32>
    %78 = torch.aten.conv2d %77, %64, %none, %69, %69, %69, %int1 : !torch.vtensor<[1,64,32,32],f32>, !torch.vtensor<[64,64,3,3],f32>, !torch.none, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.int -> !torch.vtensor<[1,64,32,32],f32>
    %79 = torch.aten.batch_norm %78, %56, %55, %55, %56, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[1,64,32,32],f32>, !torch.vtensor<[64],f32>, !torch.vtensor<[64],f32>, !torch.vtensor<[64],f32>, !torch.vtensor<[64],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[1,64,32,32],f32>
    %80 = torch.aten.relu %79 : !torch.vtensor<[1,64,32,32],f32> -> !torch.vtensor<[1,64,32,32],f32>
    %81 = torch.aten.conv2d %80, %63, %none, %69, %74, %69, %int1 : !torch.vtensor<[1,64,32,32],f32>, !torch.vtensor<[256,64,1,1],f32>, !torch.none, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.int -> !torch.vtensor<[1,256,32,32],f32>
    %82 = torch.aten.batch_norm %81, %20, %19, %19, %20, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[1,256,32,32],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[1,256,32,32],f32>
    %83 = torch.aten.conv2d %73, %62, %none, %69, %74, %69, %int1 : !torch.vtensor<[1,64,32,32],f32>, !torch.vtensor<[256,64,1,1],f32>, !torch.none, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.int -> !torch.vtensor<[1,256,32,32],f32>
    %84 = torch.aten.batch_norm %83, %20, %19, %19, %20, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[1,256,32,32],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[1,256,32,32],f32>
    %85 = torch.aten.add.Tensor %82, %84, %int1 : !torch.vtensor<[1,256,32,32],f32>, !torch.vtensor<[1,256,32,32],f32>, !torch.int -> !torch.vtensor<[1,256,32,32],f32>
    %86 = torch.aten.relu %85 : !torch.vtensor<[1,256,32,32],f32> -> !torch.vtensor<[1,256,32,32],f32>
    %87 = torch.aten.conv2d %86, %61, %none, %69, %74, %69, %int1 : !torch.vtensor<[1,256,32,32],f32>, !torch.vtensor<[64,256,1,1],f32>, !torch.none, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.int -> !torch.vtensor<[1,64,32,32],f32>
    %88 = torch.aten.batch_norm %87, %56, %55, %55, %56, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[1,64,32,32],f32>, !torch.vtensor<[64],f32>, !torch.vtensor<[64],f32>, !torch.vtensor<[64],f32>, !torch.vtensor<[64],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[1,64,32,32],f32>
    %89 = torch.aten.relu %88 : !torch.vtensor<[1,64,32,32],f32> -> !torch.vtensor<[1,64,32,32],f32>
    %90 = torch.aten.conv2d %89, %60, %none, %69, %69, %69, %int1 : !torch.vtensor<[1,64,32,32],f32>, !torch.vtensor<[64,64,3,3],f32>, !torch.none, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.int -> !torch.vtensor<[1,64,32,32],f32>
    %91 = torch.aten.batch_norm %90, %56, %55, %55, %56, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[1,64,32,32],f32>, !torch.vtensor<[64],f32>, !torch.vtensor<[64],f32>, !torch.vtensor<[64],f32>, !torch.vtensor<[64],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[1,64,32,32],f32>
    %92 = torch.aten.relu %91 : !torch.vtensor<[1,64,32,32],f32> -> !torch.vtensor<[1,64,32,32],f32>
    %93 = torch.aten.conv2d %92, %59, %none, %69, %74, %69, %int1 : !torch.vtensor<[1,64,32,32],f32>, !torch.vtensor<[256,64,1,1],f32>, !torch.none, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.int -> !torch.vtensor<[1,256,32,32],f32>
    %94 = torch.aten.batch_norm %93, %20, %19, %19, %20, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[1,256,32,32],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[1,256,32,32],f32>
    %95 = torch.aten.add.Tensor %94, %86, %int1 : !torch.vtensor<[1,256,32,32],f32>, !torch.vtensor<[1,256,32,32],f32>, !torch.int -> !torch.vtensor<[1,256,32,32],f32>
    %96 = torch.aten.relu %95 : !torch.vtensor<[1,256,32,32],f32> -> !torch.vtensor<[1,256,32,32],f32>
    %97 = torch.aten.conv2d %96, %58, %none, %69, %74, %69, %int1 : !torch.vtensor<[1,256,32,32],f32>, !torch.vtensor<[64,256,1,1],f32>, !torch.none, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.int -> !torch.vtensor<[1,64,32,32],f32>
    %98 = torch.aten.batch_norm %97, %56, %55, %55, %56, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[1,64,32,32],f32>, !torch.vtensor<[64],f32>, !torch.vtensor<[64],f32>, !torch.vtensor<[64],f32>, !torch.vtensor<[64],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[1,64,32,32],f32>
    %99 = torch.aten.relu %98 : !torch.vtensor<[1,64,32,32],f32> -> !torch.vtensor<[1,64,32,32],f32>
    %100 = torch.aten.conv2d %99, %57, %none, %69, %69, %69, %int1 : !torch.vtensor<[1,64,32,32],f32>, !torch.vtensor<[64,64,3,3],f32>, !torch.none, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.int -> !torch.vtensor<[1,64,32,32],f32>
    %101 = torch.aten.batch_norm %100, %56, %55, %55, %56, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[1,64,32,32],f32>, !torch.vtensor<[64],f32>, !torch.vtensor<[64],f32>, !torch.vtensor<[64],f32>, !torch.vtensor<[64],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[1,64,32,32],f32>
    %102 = torch.aten.relu %101 : !torch.vtensor<[1,64,32,32],f32> -> !torch.vtensor<[1,64,32,32],f32>
    %103 = torch.aten.conv2d %102, %54, %none, %69, %74, %69, %int1 : !torch.vtensor<[1,64,32,32],f32>, !torch.vtensor<[256,64,1,1],f32>, !torch.none, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.int -> !torch.vtensor<[1,256,32,32],f32>
    %104 = torch.aten.batch_norm %103, %20, %19, %19, %20, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[1,256,32,32],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[1,256,32,32],f32>
    %105 = torch.aten.add.Tensor %104, %96, %int1 : !torch.vtensor<[1,256,32,32],f32>, !torch.vtensor<[1,256,32,32],f32>, !torch.int -> !torch.vtensor<[1,256,32,32],f32>
    %106 = torch.aten.relu %105 : !torch.vtensor<[1,256,32,32],f32> -> !torch.vtensor<[1,256,32,32],f32>
    %107 = torch.aten.conv2d %106, %53, %none, %69, %74, %69, %int1 : !torch.vtensor<[1,256,32,32],f32>, !torch.vtensor<[128,256,1,1],f32>, !torch.none, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.int -> !torch.vtensor<[1,128,32,32],f32>
    %108 = torch.aten.batch_norm %107, %41, %40, %40, %41, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[1,128,32,32],f32>, !torch.vtensor<[128],f32>, !torch.vtensor<[128],f32>, !torch.vtensor<[128],f32>, !torch.vtensor<[128],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[1,128,32,32],f32>
    %109 = torch.aten.relu %108 : !torch.vtensor<[1,128,32,32],f32> -> !torch.vtensor<[1,128,32,32],f32>
    %110 = torch.aten.conv2d %109, %52, %none, %67, %69, %69, %int1 : !torch.vtensor<[1,128,32,32],f32>, !torch.vtensor<[128,128,3,3],f32>, !torch.none, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.int -> !torch.vtensor<[1,128,16,16],f32>
    %111 = torch.aten.batch_norm %110, %41, %40, %40, %41, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[1,128,16,16],f32>, !torch.vtensor<[128],f32>, !torch.vtensor<[128],f32>, !torch.vtensor<[128],f32>, !torch.vtensor<[128],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[1,128,16,16],f32>
    %112 = torch.aten.relu %111 : !torch.vtensor<[1,128,16,16],f32> -> !torch.vtensor<[1,128,16,16],f32>
    %113 = torch.aten.conv2d %112, %51, %none, %69, %74, %69, %int1 : !torch.vtensor<[1,128,16,16],f32>, !torch.vtensor<[512,128,1,1],f32>, !torch.none, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.int -> !torch.vtensor<[1,512,16,16],f32>
    %114 = torch.aten.batch_norm %113, %6, %5, %5, %6, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[1,512,16,16],f32>, !torch.vtensor<[512],f32>, !torch.vtensor<[512],f32>, !torch.vtensor<[512],f32>, !torch.vtensor<[512],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[1,512,16,16],f32>
    %115 = torch.aten.conv2d %106, %50, %none, %67, %74, %69, %int1 : !torch.vtensor<[1,256,32,32],f32>, !torch.vtensor<[512,256,1,1],f32>, !torch.none, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.int -> !torch.vtensor<[1,512,16,16],f32>
    %116 = torch.aten.batch_norm %115, %6, %5, %5, %6, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[1,512,16,16],f32>, !torch.vtensor<[512],f32>, !torch.vtensor<[512],f32>, !torch.vtensor<[512],f32>, !torch.vtensor<[512],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[1,512,16,16],f32>
    %117 = torch.aten.add.Tensor %114, %116, %int1 : !torch.vtensor<[1,512,16,16],f32>, !torch.vtensor<[1,512,16,16],f32>, !torch.int -> !torch.vtensor<[1,512,16,16],f32>
    %118 = torch.aten.relu %117 : !torch.vtensor<[1,512,16,16],f32> -> !torch.vtensor<[1,512,16,16],f32>
    %119 = torch.aten.conv2d %118, %49, %none, %69, %74, %69, %int1 : !torch.vtensor<[1,512,16,16],f32>, !torch.vtensor<[128,512,1,1],f32>, !torch.none, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.int -> !torch.vtensor<[1,128,16,16],f32>
    %120 = torch.aten.batch_norm %119, %41, %40, %40, %41, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[1,128,16,16],f32>, !torch.vtensor<[128],f32>, !torch.vtensor<[128],f32>, !torch.vtensor<[128],f32>, !torch.vtensor<[128],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[1,128,16,16],f32>
    %121 = torch.aten.relu %120 : !torch.vtensor<[1,128,16,16],f32> -> !torch.vtensor<[1,128,16,16],f32>
    %122 = torch.aten.conv2d %121, %48, %none, %69, %69, %69, %int1 : !torch.vtensor<[1,128,16,16],f32>, !torch.vtensor<[128,128,3,3],f32>, !torch.none, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.int -> !torch.vtensor<[1,128,16,16],f32>
    %123 = torch.aten.batch_norm %122, %41, %40, %40, %41, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[1,128,16,16],f32>, !torch.vtensor<[128],f32>, !torch.vtensor<[128],f32>, !torch.vtensor<[128],f32>, !torch.vtensor<[128],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[1,128,16,16],f32>
    %124 = torch.aten.relu %123 : !torch.vtensor<[1,128,16,16],f32> -> !torch.vtensor<[1,128,16,16],f32>
    %125 = torch.aten.conv2d %124, %47, %none, %69, %74, %69, %int1 : !torch.vtensor<[1,128,16,16],f32>, !torch.vtensor<[512,128,1,1],f32>, !torch.none, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.int -> !torch.vtensor<[1,512,16,16],f32>
    %126 = torch.aten.batch_norm %125, %6, %5, %5, %6, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[1,512,16,16],f32>, !torch.vtensor<[512],f32>, !torch.vtensor<[512],f32>, !torch.vtensor<[512],f32>, !torch.vtensor<[512],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[1,512,16,16],f32>
    %127 = torch.aten.add.Tensor %126, %118, %int1 : !torch.vtensor<[1,512,16,16],f32>, !torch.vtensor<[1,512,16,16],f32>, !torch.int -> !torch.vtensor<[1,512,16,16],f32>
    %128 = torch.aten.relu %127 : !torch.vtensor<[1,512,16,16],f32> -> !torch.vtensor<[1,512,16,16],f32>
    %129 = torch.aten.conv2d %128, %46, %none, %69, %74, %69, %int1 : !torch.vtensor<[1,512,16,16],f32>, !torch.vtensor<[128,512,1,1],f32>, !torch.none, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.int -> !torch.vtensor<[1,128,16,16],f32>
    %130 = torch.aten.batch_norm %129, %41, %40, %40, %41, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[1,128,16,16],f32>, !torch.vtensor<[128],f32>, !torch.vtensor<[128],f32>, !torch.vtensor<[128],f32>, !torch.vtensor<[128],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[1,128,16,16],f32>
    %131 = torch.aten.relu %130 : !torch.vtensor<[1,128,16,16],f32> -> !torch.vtensor<[1,128,16,16],f32>
    %132 = torch.aten.conv2d %131, %45, %none, %69, %69, %69, %int1 : !torch.vtensor<[1,128,16,16],f32>, !torch.vtensor<[128,128,3,3],f32>, !torch.none, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.int -> !torch.vtensor<[1,128,16,16],f32>
    %133 = torch.aten.batch_norm %132, %41, %40, %40, %41, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[1,128,16,16],f32>, !torch.vtensor<[128],f32>, !torch.vtensor<[128],f32>, !torch.vtensor<[128],f32>, !torch.vtensor<[128],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[1,128,16,16],f32>
    %134 = torch.aten.relu %133 : !torch.vtensor<[1,128,16,16],f32> -> !torch.vtensor<[1,128,16,16],f32>
    %135 = torch.aten.conv2d %134, %44, %none, %69, %74, %69, %int1 : !torch.vtensor<[1,128,16,16],f32>, !torch.vtensor<[512,128,1,1],f32>, !torch.none, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.int -> !torch.vtensor<[1,512,16,16],f32>
    %136 = torch.aten.batch_norm %135, %6, %5, %5, %6, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[1,512,16,16],f32>, !torch.vtensor<[512],f32>, !torch.vtensor<[512],f32>, !torch.vtensor<[512],f32>, !torch.vtensor<[512],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[1,512,16,16],f32>
    %137 = torch.aten.add.Tensor %136, %128, %int1 : !torch.vtensor<[1,512,16,16],f32>, !torch.vtensor<[1,512,16,16],f32>, !torch.int -> !torch.vtensor<[1,512,16,16],f32>
    %138 = torch.aten.relu %137 : !torch.vtensor<[1,512,16,16],f32> -> !torch.vtensor<[1,512,16,16],f32>
    %139 = torch.aten.conv2d %138, %43, %none, %69, %74, %69, %int1 : !torch.vtensor<[1,512,16,16],f32>, !torch.vtensor<[128,512,1,1],f32>, !torch.none, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.int -> !torch.vtensor<[1,128,16,16],f32>
    %140 = torch.aten.batch_norm %139, %41, %40, %40, %41, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[1,128,16,16],f32>, !torch.vtensor<[128],f32>, !torch.vtensor<[128],f32>, !torch.vtensor<[128],f32>, !torch.vtensor<[128],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[1,128,16,16],f32>
    %141 = torch.aten.relu %140 : !torch.vtensor<[1,128,16,16],f32> -> !torch.vtensor<[1,128,16,16],f32>
    %142 = torch.aten.conv2d %141, %42, %none, %69, %69, %69, %int1 : !torch.vtensor<[1,128,16,16],f32>, !torch.vtensor<[128,128,3,3],f32>, !torch.none, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.int -> !torch.vtensor<[1,128,16,16],f32>
    %143 = torch.aten.batch_norm %142, %41, %40, %40, %41, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[1,128,16,16],f32>, !torch.vtensor<[128],f32>, !torch.vtensor<[128],f32>, !torch.vtensor<[128],f32>, !torch.vtensor<[128],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[1,128,16,16],f32>
    %144 = torch.aten.relu %143 : !torch.vtensor<[1,128,16,16],f32> -> !torch.vtensor<[1,128,16,16],f32>
    %145 = torch.aten.conv2d %144, %39, %none, %69, %74, %69, %int1 : !torch.vtensor<[1,128,16,16],f32>, !torch.vtensor<[512,128,1,1],f32>, !torch.none, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.int -> !torch.vtensor<[1,512,16,16],f32>
    %146 = torch.aten.batch_norm %145, %6, %5, %5, %6, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[1,512,16,16],f32>, !torch.vtensor<[512],f32>, !torch.vtensor<[512],f32>, !torch.vtensor<[512],f32>, !torch.vtensor<[512],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[1,512,16,16],f32>
    %147 = torch.aten.add.Tensor %146, %138, %int1 : !torch.vtensor<[1,512,16,16],f32>, !torch.vtensor<[1,512,16,16],f32>, !torch.int -> !torch.vtensor<[1,512,16,16],f32>
    %148 = torch.aten.relu %147 : !torch.vtensor<[1,512,16,16],f32> -> !torch.vtensor<[1,512,16,16],f32>
    %149 = torch.aten.conv2d %148, %38, %none, %69, %74, %69, %int1 : !torch.vtensor<[1,512,16,16],f32>, !torch.vtensor<[256,512,1,1],f32>, !torch.none, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.int -> !torch.vtensor<[1,256,16,16],f32>
    %150 = torch.aten.batch_norm %149, %20, %19, %19, %20, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[1,256,16,16],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[1,256,16,16],f32>
    %151 = torch.aten.relu %150 : !torch.vtensor<[1,256,16,16],f32> -> !torch.vtensor<[1,256,16,16],f32>
    %152 = torch.aten.conv2d %151, %37, %none, %67, %69, %69, %int1 : !torch.vtensor<[1,256,16,16],f32>, !torch.vtensor<[256,256,3,3],f32>, !torch.none, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.int -> !torch.vtensor<[1,256,8,8],f32>
    %153 = torch.aten.batch_norm %152, %20, %19, %19, %20, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[1,256,8,8],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[1,256,8,8],f32>
    %154 = torch.aten.relu %153 : !torch.vtensor<[1,256,8,8],f32> -> !torch.vtensor<[1,256,8,8],f32>
    %155 = torch.aten.conv2d %154, %36, %none, %69, %74, %69, %int1 : !torch.vtensor<[1,256,8,8],f32>, !torch.vtensor<[1024,256,1,1],f32>, !torch.none, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.int -> !torch.vtensor<[1,1024,8,8],f32>
    %156 = torch.aten.batch_norm %155, %17, %16, %16, %17, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[1,1024,8,8],f32>, !torch.vtensor<[1024],f32>, !torch.vtensor<[1024],f32>, !torch.vtensor<[1024],f32>, !torch.vtensor<[1024],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[1,1024,8,8],f32>
    %157 = torch.aten.conv2d %148, %35, %none, %67, %74, %69, %int1 : !torch.vtensor<[1,512,16,16],f32>, !torch.vtensor<[1024,512,1,1],f32>, !torch.none, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.int -> !torch.vtensor<[1,1024,8,8],f32>
    %158 = torch.aten.batch_norm %157, %17, %16, %16, %17, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[1,1024,8,8],f32>, !torch.vtensor<[1024],f32>, !torch.vtensor<[1024],f32>, !torch.vtensor<[1024],f32>, !torch.vtensor<[1024],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[1,1024,8,8],f32>
    %159 = torch.aten.add.Tensor %156, %158, %int1 : !torch.vtensor<[1,1024,8,8],f32>, !torch.vtensor<[1,1024,8,8],f32>, !torch.int -> !torch.vtensor<[1,1024,8,8],f32>
    %160 = torch.aten.relu %159 : !torch.vtensor<[1,1024,8,8],f32> -> !torch.vtensor<[1,1024,8,8],f32>
    %161 = torch.aten.conv2d %160, %34, %none, %69, %74, %69, %int1 : !torch.vtensor<[1,1024,8,8],f32>, !torch.vtensor<[256,1024,1,1],f32>, !torch.none, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.int -> !torch.vtensor<[1,256,8,8],f32>
    %162 = torch.aten.batch_norm %161, %20, %19, %19, %20, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[1,256,8,8],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[1,256,8,8],f32>
    %163 = torch.aten.relu %162 : !torch.vtensor<[1,256,8,8],f32> -> !torch.vtensor<[1,256,8,8],f32>
    %164 = torch.aten.conv2d %163, %33, %none, %69, %69, %69, %int1 : !torch.vtensor<[1,256,8,8],f32>, !torch.vtensor<[256,256,3,3],f32>, !torch.none, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.int -> !torch.vtensor<[1,256,8,8],f32>
    %165 = torch.aten.batch_norm %164, %20, %19, %19, %20, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[1,256,8,8],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[1,256,8,8],f32>
    %166 = torch.aten.relu %165 : !torch.vtensor<[1,256,8,8],f32> -> !torch.vtensor<[1,256,8,8],f32>
    %167 = torch.aten.conv2d %166, %32, %none, %69, %74, %69, %int1 : !torch.vtensor<[1,256,8,8],f32>, !torch.vtensor<[1024,256,1,1],f32>, !torch.none, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.int -> !torch.vtensor<[1,1024,8,8],f32>
    %168 = torch.aten.batch_norm %167, %17, %16, %16, %17, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[1,1024,8,8],f32>, !torch.vtensor<[1024],f32>, !torch.vtensor<[1024],f32>, !torch.vtensor<[1024],f32>, !torch.vtensor<[1024],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[1,1024,8,8],f32>
    %169 = torch.aten.add.Tensor %168, %160, %int1 : !torch.vtensor<[1,1024,8,8],f32>, !torch.vtensor<[1,1024,8,8],f32>, !torch.int -> !torch.vtensor<[1,1024,8,8],f32>
    %170 = torch.aten.relu %169 : !torch.vtensor<[1,1024,8,8],f32> -> !torch.vtensor<[1,1024,8,8],f32>
    %171 = torch.aten.conv2d %170, %31, %none, %69, %74, %69, %int1 : !torch.vtensor<[1,1024,8,8],f32>, !torch.vtensor<[256,1024,1,1],f32>, !torch.none, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.int -> !torch.vtensor<[1,256,8,8],f32>
    %172 = torch.aten.batch_norm %171, %20, %19, %19, %20, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[1,256,8,8],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[1,256,8,8],f32>
    %173 = torch.aten.relu %172 : !torch.vtensor<[1,256,8,8],f32> -> !torch.vtensor<[1,256,8,8],f32>
    %174 = torch.aten.conv2d %173, %30, %none, %69, %69, %69, %int1 : !torch.vtensor<[1,256,8,8],f32>, !torch.vtensor<[256,256,3,3],f32>, !torch.none, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.int -> !torch.vtensor<[1,256,8,8],f32>
    %175 = torch.aten.batch_norm %174, %20, %19, %19, %20, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[1,256,8,8],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[1,256,8,8],f32>
    %176 = torch.aten.relu %175 : !torch.vtensor<[1,256,8,8],f32> -> !torch.vtensor<[1,256,8,8],f32>
    %177 = torch.aten.conv2d %176, %29, %none, %69, %74, %69, %int1 : !torch.vtensor<[1,256,8,8],f32>, !torch.vtensor<[1024,256,1,1],f32>, !torch.none, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.int -> !torch.vtensor<[1,1024,8,8],f32>
    %178 = torch.aten.batch_norm %177, %17, %16, %16, %17, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[1,1024,8,8],f32>, !torch.vtensor<[1024],f32>, !torch.vtensor<[1024],f32>, !torch.vtensor<[1024],f32>, !torch.vtensor<[1024],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[1,1024,8,8],f32>
    %179 = torch.aten.add.Tensor %178, %170, %int1 : !torch.vtensor<[1,1024,8,8],f32>, !torch.vtensor<[1,1024,8,8],f32>, !torch.int -> !torch.vtensor<[1,1024,8,8],f32>
    %180 = torch.aten.relu %179 : !torch.vtensor<[1,1024,8,8],f32> -> !torch.vtensor<[1,1024,8,8],f32>
    %181 = torch.aten.conv2d %180, %28, %none, %69, %74, %69, %int1 : !torch.vtensor<[1,1024,8,8],f32>, !torch.vtensor<[256,1024,1,1],f32>, !torch.none, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.int -> !torch.vtensor<[1,256,8,8],f32>
    %182 = torch.aten.batch_norm %181, %20, %19, %19, %20, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[1,256,8,8],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[1,256,8,8],f32>
    %183 = torch.aten.relu %182 : !torch.vtensor<[1,256,8,8],f32> -> !torch.vtensor<[1,256,8,8],f32>
    %184 = torch.aten.conv2d %183, %27, %none, %69, %69, %69, %int1 : !torch.vtensor<[1,256,8,8],f32>, !torch.vtensor<[256,256,3,3],f32>, !torch.none, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.int -> !torch.vtensor<[1,256,8,8],f32>
    %185 = torch.aten.batch_norm %184, %20, %19, %19, %20, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[1,256,8,8],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[1,256,8,8],f32>
    %186 = torch.aten.relu %185 : !torch.vtensor<[1,256,8,8],f32> -> !torch.vtensor<[1,256,8,8],f32>
    %187 = torch.aten.conv2d %186, %26, %none, %69, %74, %69, %int1 : !torch.vtensor<[1,256,8,8],f32>, !torch.vtensor<[1024,256,1,1],f32>, !torch.none, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.int -> !torch.vtensor<[1,1024,8,8],f32>
    %188 = torch.aten.batch_norm %187, %17, %16, %16, %17, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[1,1024,8,8],f32>, !torch.vtensor<[1024],f32>, !torch.vtensor<[1024],f32>, !torch.vtensor<[1024],f32>, !torch.vtensor<[1024],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[1,1024,8,8],f32>
    %189 = torch.aten.add.Tensor %188, %180, %int1 : !torch.vtensor<[1,1024,8,8],f32>, !torch.vtensor<[1,1024,8,8],f32>, !torch.int -> !torch.vtensor<[1,1024,8,8],f32>
    %190 = torch.aten.relu %189 : !torch.vtensor<[1,1024,8,8],f32> -> !torch.vtensor<[1,1024,8,8],f32>
    %191 = torch.aten.conv2d %190, %25, %none, %69, %74, %69, %int1 : !torch.vtensor<[1,1024,8,8],f32>, !torch.vtensor<[256,1024,1,1],f32>, !torch.none, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.int -> !torch.vtensor<[1,256,8,8],f32>
    %192 = torch.aten.batch_norm %191, %20, %19, %19, %20, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[1,256,8,8],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[1,256,8,8],f32>
    %193 = torch.aten.relu %192 : !torch.vtensor<[1,256,8,8],f32> -> !torch.vtensor<[1,256,8,8],f32>
    %194 = torch.aten.conv2d %193, %24, %none, %69, %69, %69, %int1 : !torch.vtensor<[1,256,8,8],f32>, !torch.vtensor<[256,256,3,3],f32>, !torch.none, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.int -> !torch.vtensor<[1,256,8,8],f32>
    %195 = torch.aten.batch_norm %194, %20, %19, %19, %20, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[1,256,8,8],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[1,256,8,8],f32>
    %196 = torch.aten.relu %195 : !torch.vtensor<[1,256,8,8],f32> -> !torch.vtensor<[1,256,8,8],f32>
    %197 = torch.aten.conv2d %196, %23, %none, %69, %74, %69, %int1 : !torch.vtensor<[1,256,8,8],f32>, !torch.vtensor<[1024,256,1,1],f32>, !torch.none, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.int -> !torch.vtensor<[1,1024,8,8],f32>
    %198 = torch.aten.batch_norm %197, %17, %16, %16, %17, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[1,1024,8,8],f32>, !torch.vtensor<[1024],f32>, !torch.vtensor<[1024],f32>, !torch.vtensor<[1024],f32>, !torch.vtensor<[1024],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[1,1024,8,8],f32>
    %199 = torch.aten.add.Tensor %198, %190, %int1 : !torch.vtensor<[1,1024,8,8],f32>, !torch.vtensor<[1,1024,8,8],f32>, !torch.int -> !torch.vtensor<[1,1024,8,8],f32>
    %200 = torch.aten.relu %199 : !torch.vtensor<[1,1024,8,8],f32> -> !torch.vtensor<[1,1024,8,8],f32>
    %201 = torch.aten.conv2d %200, %22, %none, %69, %74, %69, %int1 : !torch.vtensor<[1,1024,8,8],f32>, !torch.vtensor<[256,1024,1,1],f32>, !torch.none, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.int -> !torch.vtensor<[1,256,8,8],f32>
    %202 = torch.aten.batch_norm %201, %20, %19, %19, %20, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[1,256,8,8],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[1,256,8,8],f32>
    %203 = torch.aten.relu %202 : !torch.vtensor<[1,256,8,8],f32> -> !torch.vtensor<[1,256,8,8],f32>
    %204 = torch.aten.conv2d %203, %21, %none, %69, %69, %69, %int1 : !torch.vtensor<[1,256,8,8],f32>, !torch.vtensor<[256,256,3,3],f32>, !torch.none, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.int -> !torch.vtensor<[1,256,8,8],f32>
    %205 = torch.aten.batch_norm %204, %20, %19, %19, %20, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[1,256,8,8],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.vtensor<[256],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[1,256,8,8],f32>
    %206 = torch.aten.relu %205 : !torch.vtensor<[1,256,8,8],f32> -> !torch.vtensor<[1,256,8,8],f32>
    %207 = torch.aten.conv2d %206, %18, %none, %69, %74, %69, %int1 : !torch.vtensor<[1,256,8,8],f32>, !torch.vtensor<[1024,256,1,1],f32>, !torch.none, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.int -> !torch.vtensor<[1,1024,8,8],f32>
    %208 = torch.aten.batch_norm %207, %17, %16, %16, %17, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[1,1024,8,8],f32>, !torch.vtensor<[1024],f32>, !torch.vtensor<[1024],f32>, !torch.vtensor<[1024],f32>, !torch.vtensor<[1024],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[1,1024,8,8],f32>
    %209 = torch.aten.add.Tensor %208, %200, %int1 : !torch.vtensor<[1,1024,8,8],f32>, !torch.vtensor<[1,1024,8,8],f32>, !torch.int -> !torch.vtensor<[1,1024,8,8],f32>
    %210 = torch.aten.relu %209 : !torch.vtensor<[1,1024,8,8],f32> -> !torch.vtensor<[1,1024,8,8],f32>
    %211 = torch.aten.conv2d %210, %15, %none, %69, %74, %69, %int1 : !torch.vtensor<[1,1024,8,8],f32>, !torch.vtensor<[512,1024,1,1],f32>, !torch.none, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.int -> !torch.vtensor<[1,512,8,8],f32>
    %212 = torch.aten.batch_norm %211, %6, %5, %5, %6, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[1,512,8,8],f32>, !torch.vtensor<[512],f32>, !torch.vtensor<[512],f32>, !torch.vtensor<[512],f32>, !torch.vtensor<[512],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[1,512,8,8],f32>
    %213 = torch.aten.relu %212 : !torch.vtensor<[1,512,8,8],f32> -> !torch.vtensor<[1,512,8,8],f32>
    %214 = torch.aten.conv2d %213, %14, %none, %67, %69, %69, %int1 : !torch.vtensor<[1,512,8,8],f32>, !torch.vtensor<[512,512,3,3],f32>, !torch.none, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.int -> !torch.vtensor<[1,512,4,4],f32>
    %215 = torch.aten.batch_norm %214, %6, %5, %5, %6, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[1,512,4,4],f32>, !torch.vtensor<[512],f32>, !torch.vtensor<[512],f32>, !torch.vtensor<[512],f32>, !torch.vtensor<[512],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[1,512,4,4],f32>
    %216 = torch.aten.relu %215 : !torch.vtensor<[1,512,4,4],f32> -> !torch.vtensor<[1,512,4,4],f32>
    %217 = torch.aten.conv2d %216, %13, %none, %69, %74, %69, %int1 : !torch.vtensor<[1,512,4,4],f32>, !torch.vtensor<[2048,512,1,1],f32>, !torch.none, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.int -> !torch.vtensor<[1,2048,4,4],f32>
    %218 = torch.aten.batch_norm %217, %3, %2, %2, %3, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[1,2048,4,4],f32>, !torch.vtensor<[2048],f32>, !torch.vtensor<[2048],f32>, !torch.vtensor<[2048],f32>, !torch.vtensor<[2048],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[1,2048,4,4],f32>
    %219 = torch.aten.conv2d %210, %12, %none, %67, %74, %69, %int1 : !torch.vtensor<[1,1024,8,8],f32>, !torch.vtensor<[2048,1024,1,1],f32>, !torch.none, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.int -> !torch.vtensor<[1,2048,4,4],f32>
    %220 = torch.aten.batch_norm %219, %3, %2, %2, %3, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[1,2048,4,4],f32>, !torch.vtensor<[2048],f32>, !torch.vtensor<[2048],f32>, !torch.vtensor<[2048],f32>, !torch.vtensor<[2048],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[1,2048,4,4],f32>
    %221 = torch.aten.add.Tensor %218, %220, %int1 : !torch.vtensor<[1,2048,4,4],f32>, !torch.vtensor<[1,2048,4,4],f32>, !torch.int -> !torch.vtensor<[1,2048,4,4],f32>
    %222 = torch.aten.relu %221 : !torch.vtensor<[1,2048,4,4],f32> -> !torch.vtensor<[1,2048,4,4],f32>
    %223 = torch.aten.conv2d %222, %11, %none, %69, %74, %69, %int1 : !torch.vtensor<[1,2048,4,4],f32>, !torch.vtensor<[512,2048,1,1],f32>, !torch.none, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.int -> !torch.vtensor<[1,512,4,4],f32>
    %224 = torch.aten.batch_norm %223, %6, %5, %5, %6, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[1,512,4,4],f32>, !torch.vtensor<[512],f32>, !torch.vtensor<[512],f32>, !torch.vtensor<[512],f32>, !torch.vtensor<[512],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[1,512,4,4],f32>
    %225 = torch.aten.relu %224 : !torch.vtensor<[1,512,4,4],f32> -> !torch.vtensor<[1,512,4,4],f32>
    %226 = torch.aten.conv2d %225, %10, %none, %69, %69, %69, %int1 : !torch.vtensor<[1,512,4,4],f32>, !torch.vtensor<[512,512,3,3],f32>, !torch.none, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.int -> !torch.vtensor<[1,512,4,4],f32>
    %227 = torch.aten.batch_norm %226, %6, %5, %5, %6, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[1,512,4,4],f32>, !torch.vtensor<[512],f32>, !torch.vtensor<[512],f32>, !torch.vtensor<[512],f32>, !torch.vtensor<[512],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[1,512,4,4],f32>
    %228 = torch.aten.relu %227 : !torch.vtensor<[1,512,4,4],f32> -> !torch.vtensor<[1,512,4,4],f32>
    %229 = torch.aten.conv2d %228, %9, %none, %69, %74, %69, %int1 : !torch.vtensor<[1,512,4,4],f32>, !torch.vtensor<[2048,512,1,1],f32>, !torch.none, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.int -> !torch.vtensor<[1,2048,4,4],f32>
    %230 = torch.aten.batch_norm %229, %3, %2, %2, %3, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[1,2048,4,4],f32>, !torch.vtensor<[2048],f32>, !torch.vtensor<[2048],f32>, !torch.vtensor<[2048],f32>, !torch.vtensor<[2048],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[1,2048,4,4],f32>
    %231 = torch.aten.add.Tensor %230, %222, %int1 : !torch.vtensor<[1,2048,4,4],f32>, !torch.vtensor<[1,2048,4,4],f32>, !torch.int -> !torch.vtensor<[1,2048,4,4],f32>
    %232 = torch.aten.relu %231 : !torch.vtensor<[1,2048,4,4],f32> -> !torch.vtensor<[1,2048,4,4],f32>
    %233 = torch.aten.conv2d %232, %8, %none, %69, %74, %69, %int1 : !torch.vtensor<[1,2048,4,4],f32>, !torch.vtensor<[512,2048,1,1],f32>, !torch.none, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.int -> !torch.vtensor<[1,512,4,4],f32>
    %234 = torch.aten.batch_norm %233, %6, %5, %5, %6, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[1,512,4,4],f32>, !torch.vtensor<[512],f32>, !torch.vtensor<[512],f32>, !torch.vtensor<[512],f32>, !torch.vtensor<[512],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[1,512,4,4],f32>
    %235 = torch.aten.relu %234 : !torch.vtensor<[1,512,4,4],f32> -> !torch.vtensor<[1,512,4,4],f32>
    %236 = torch.aten.conv2d %235, %7, %none, %69, %69, %69, %int1 : !torch.vtensor<[1,512,4,4],f32>, !torch.vtensor<[512,512,3,3],f32>, !torch.none, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.int -> !torch.vtensor<[1,512,4,4],f32>
    %237 = torch.aten.batch_norm %236, %6, %5, %5, %6, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[1,512,4,4],f32>, !torch.vtensor<[512],f32>, !torch.vtensor<[512],f32>, !torch.vtensor<[512],f32>, !torch.vtensor<[512],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[1,512,4,4],f32>
    %238 = torch.aten.relu %237 : !torch.vtensor<[1,512,4,4],f32> -> !torch.vtensor<[1,512,4,4],f32>
    %239 = torch.aten.conv2d %238, %4, %none, %69, %74, %69, %int1 : !torch.vtensor<[1,512,4,4],f32>, !torch.vtensor<[2048,512,1,1],f32>, !torch.none, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.int -> !torch.vtensor<[1,2048,4,4],f32>
    %240 = torch.aten.batch_norm %239, %3, %2, %2, %3, %false, %float1.000000e-01, %float1.000000e-05, %true : !torch.vtensor<[1,2048,4,4],f32>, !torch.vtensor<[2048],f32>, !torch.vtensor<[2048],f32>, !torch.vtensor<[2048],f32>, !torch.vtensor<[2048],f32>, !torch.bool, !torch.float, !torch.float, !torch.bool -> !torch.vtensor<[1,2048,4,4],f32>
    %241 = torch.aten.add.Tensor %240, %232, %int1 : !torch.vtensor<[1,2048,4,4],f32>, !torch.vtensor<[1,2048,4,4],f32>, !torch.int -> !torch.vtensor<[1,2048,4,4],f32>
    %242 = torch.aten.relu %241 : !torch.vtensor<[1,2048,4,4],f32> -> !torch.vtensor<[1,2048,4,4],f32>
    %243 = torch.aten.adaptive_avg_pool2d %242, %69 : !torch.vtensor<[1,2048,4,4],f32>, !torch.list<int> -> !torch.vtensor<[1,2048,1,1],f32>
    %244 = torch.aten.flatten.using_ints %243, %int1, %int-1 : !torch.vtensor<[1,2048,1,1],f32>, !torch.int, !torch.int -> !torch.vtensor<[1,2048],f32>
    %245 = torch.aten.linear %244, %1, %0 : !torch.vtensor<[1,2048],f32>, !torch.vtensor<[1000,2048],f32>, !torch.vtensor<[1000],f32> -> !torch.vtensor<[1,1000],f32>
    %246 = torch.aten.softmax.int %245, %int-1, %none : !torch.vtensor<[1,1000],f32>, !torch.int, !torch.none -> !torch.vtensor<[1,1000],f32>
    return %246 : !torch.vtensor<[1,1000],f32>
  }
}

